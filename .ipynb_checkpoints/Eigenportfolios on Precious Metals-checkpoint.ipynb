{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs Trading using Eigenportfolios\n",
    "\n",
    "This analysis shows an application of Principal Component Analysis in order to develop a pairs trading strategy to capture statistical arbitrage in the Precious Metals Market. \n",
    "\n",
    "We are going to look at how using PCA to create \"eigenportfolios\" can work as an alternative to using the standard method of looking for cointegration and mean reversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5128084fd30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstattools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.tsa.stattools as ts \n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the Precious Metals Datase\n",
    "\n",
    "This dataset deals with the london fix spot prices for gold, silver, platinum, and palladium. \n",
    "\n",
    "It was taken from: \n",
    "https://www.perthmint.com/historical_metal_prices.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Old prices of gold dating back to 1922 - 2016\n",
    "\n",
    "df_old = pd.read_csv('londonfixes.csv', error_bad_lines=False)\n",
    "df_old = df_old.drop(df_old.columns[8:], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to grab dates in the past 10 years, i.e. 2008 - 2018. In the old dataset, we only want to grab it to eoy 2015 from the old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_old.loc[14628]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_old.loc[17530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = df_old.loc[14628:17530].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The dataset has a lot of erroneous fields\n",
    "df_cur = pd.read_csv('londonfixes-current.csv', error_bad_lines=False)\n",
    "df_cur = df_cur.drop(df_cur.columns[8:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cur = df_cur.append(df_old, ignore_index=True)\n",
    "df_cur = df_cur.rename(index=str, columns={df_cur.columns[i]:df_cur.loc[0][df_cur.columns[i]] for i in range(len(df_cur.columns))})\n",
    "df_cur = df_cur.rename(index=str, columns={'Metal/Currency': 'Date'})\n",
    "df_cur.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The london fix has an AM and PM fix for gold, platinum, and palladium, while it only has 1 fix for silver. \n",
    "\n",
    "Since we need to treat the AM and PM fix as different events, we separate this so that each fix represents a row in the dataset. Since Silver only has one fix, we will duplicate its fix price for both the AM and PM value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cur.columns=['Date', 'gold-am', 'gold-pm', 'silver-am', 'plat-am', 'plat-pm', 'pal-am', 'pal-pm']\n",
    "# Get rid of 4 columns of metadata\n",
    "df_cur = df_cur[4:]\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "# Mask out the first 4 rows that have metadata\n",
    "df_new['date'] = pd.to_datetime(df_cur['Date'][4:], dayfirst=True)\n",
    "df_new\n",
    "\n",
    "# Clean the columns in the old dataset\n",
    "# For some reason, calling .str.replace() on silver and palladium sometimes makes it all go to NaN. \n",
    "# We think this is some issue with Pandas because the column is of mixed types (integers and floats)\n",
    "df_new['gold'] = pd.to_numeric(df_cur['gold-am'].str.replace(',',''))\n",
    "df_new['plat'] = pd.to_numeric(df_cur['plat-am'].str.replace(',',''))\n",
    "# Use Silver's value for BOTH AM and PM because it only has 1 fix price\n",
    "df_new['silver'] = pd.to_numeric(df_cur['silver-am'])\n",
    "df_new['pal'] = pd.to_numeric(df_cur['pal-am'])\n",
    "\n",
    "# Do the same exact cleaning process for the current dataset\n",
    "df_fake = df_new.copy(deep=True)\n",
    "df_fake.date = df_fake.date+pd.Timedelta('8 hours')\n",
    "df_fake['gold'] = pd.to_numeric(df_cur['gold-pm'].str.replace(',',''))\n",
    "df_fake['plat'] = pd.to_numeric(df_cur['plat-pm'].str.replace(',',''))\n",
    "# Use Silver's value for BOTH AM and PM because it only has 1 fix price\n",
    "df_fake['silver'] = pd.to_numeric(df_cur['silver-am'])\n",
    "df_fake['pal'] = pd.to_numeric(df_cur['pal-pm'].str.replace(',','').str.replace(' ', ''))\n",
    "\n",
    "df_new = df_new.append(df_fake, ignore_index=True)\n",
    "df_new = df_new.sort_values('date').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df_new['date'], df_new['gold'], color='yellow', label='gold', alpha=0.7)\n",
    "plt.plot(df_new['date'], df_new['silver'], color='gray', label='silver', alpha=0.7)\n",
    "plt.plot(df_new['date'], df_new['plat'], color='green', label='platinum', alpha=0.7)\n",
    "plt.plot(df_new['date'], df_new['pal'], color='red', label='palladium', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.ylim([0, 3000])\n",
    "\n",
    "plt.title('Historical London Precious Metal Spot Prices', fontsize=24)\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the drastic spikes in Palladium in 2012. \n",
    "\n",
    "We think this is due to some documentation error or some fluke, so we will exclude these values from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_new.iloc[3000:6000].query('pal < 250'))\n",
    "df_new.at[df_new.iloc[3000:6000].query('pal < 250').index, 'pal'] = np.nan\n",
    "print(df_new.iloc[3000:6000].query('pal < 250'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Cointegration/Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engle–Granger two-step method\n",
    "\n",
    "If $x_t$ and $y_t$ are non-stationary and cointegrated, then a linear combination of them must be stationary. In other words:\n",
    "\n",
    "$y_t − \\beta x_t = u_t$ \n",
    "\n",
    "where $u_{t}$ is stationary.\n",
    "\n",
    "If we knew $u_{t}$,  we could just test it for stationarity with something like a Dickey–Fuller test, Phillips–Perron test and be done. But because we don't know $u_{t}$, we must estimate this first, generally by using ordinary least squares, and then run our stationarity test on the estimated $u_{t}$ series, often denoted $\\hat u_t$\n",
    "\n",
    "A second regression is then run on the first differenced variables from the first regression, and the lagged residuals $\\hat{{u}}_{t-1}$ is included as a regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Performs unit root test and cointegration test for all pairs of metals\n",
    "\n",
    "for metal1 in df_new.columns[2:]:\n",
    "    for metal2 in df_new.columns[2:]:\n",
    "        if metal1 == metal2:\n",
    "            continue\n",
    "        df_comp = df_new.query('{0} == {1} and {2} == {3}'.format(metal1, metal1, metal2, metal2))\n",
    "        print('Cointegration Test for %s and %s' % (metal1, metal2))\n",
    "        coint, pval, crit = ts.coint(df_comp[metal1], df_comp[metal2])\n",
    "        print('Cointegration T-statistic: %0.2f' % coint)\n",
    "        print('P-value: %0.5f' % pval)\n",
    "        if pval < 0.05:\n",
    "            print('SIGNIFICANT')\n",
    "        else:\n",
    "            print('INSIGNIFICANT')\n",
    "        print('============================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenportfolio PCA Approach\n",
    "\n",
    "We were unable to develop a pairs trading strategy using cointegration, so we will explore an alternative strategy. \n",
    "\n",
    "We are now going to use PCA (Principal Component Analysis) in order to decompose this historical market. \n",
    "\n",
    "PCA is a popular technique used in data science to reduce the dimensionality of the data while retaining as much information of the data as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of Principal Component Analysis\n",
    "\n",
    "We now perform Singular Value Decomposition of the Covariance Matrix. \n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{\\mathbf{A^{T}A}}_{W \\times D} = \\underbrace{\\mathbf{U}}_{W \\times W} \\times \\underbrace{\\mathbf{\\Sigma}}_{W\\times D} \\times \\underbrace{\\mathbf{V}^{\\text{T}}}_{D \\times D} = \n",
    "\\end{equation}\n",
    "\n",
    "Where the $\\mathbf{U}$ corresponds to the Left Principal Components, $\\mathbf{\\sigma}$ corresponds to the diagonlized Eigenvalues in ascending order, and $\\mathbf{V^T}$ corresponds to the Right Principal Components.\n",
    "\n",
    "Theoretically, PCA tries to project a $n$-dimensional space into a smaller $d$-dimensionsal space, where $n >> d$. \n",
    "\n",
    "It does this by decomposing the co-variance matrix, $\\mathbf{A^{T}A}$ which is the matrix of datapoints whose $ij$th element is the covariance between the $i$th and $j$th element in a random vector. \n",
    "\n",
    "The principal components of the decomposition of the covariance matrix that capture the most information of the dataset whenever the dataset is projected onto that vector. The Eigenvalue that corresponds to that eigenvector is how strong that eigenvector explains the variance of the data. The left singular vectors tell us how much each particular row accounts for the variance of the data, while the right singular vectors tell us how much each particular column to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Eigenvalues and Eigenvectors\n",
    "\n",
    "Following from our knowledge of what PCA does fundamentally to an arbitrary dataset, how can we interpret it when applied to trading data?\n",
    "\n",
    "Suppose we have a matrix $\\mathbf{A \\in \\mathbb{R}^{mxn}}$ that represents a basket of $m$ stocks over $n$ days. Whenever we take take the Principal Component Analysis of this matrix, we are left with a decomposition of the left principal components multiplied by the eigenvales multiplied by the right principal vectors. \n",
    "\n",
    "The eigenvalues represent how much of the variance of the dataset is captured in each \"concept\" of the dataset. \n",
    "\n",
    "The left eigenvectors can be interpreted as how much the rows (a.k.a the stocks) correspond to each concept. \n",
    "\n",
    "The right eigenvectors can be interpreted as how much the columns (a.k.a the daily prices) correspond to each concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_svd = df_new[['date', 'gold', 'silver', 'plat', 'pal']]\n",
    "\n",
    "# Clean out days where all indices have no entries (Holidays, Weekends)\n",
    "df_svd = df_svd[~df_svd.iloc[:, 1:].isnull().all(1)]\n",
    "\n",
    "df_svd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the price data to percent returns (current price - previous price)/previous price. \n",
    "\n",
    "We do this because PCA is sensitive to scale, and this serves as a form of normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_daily_returns_matrix(df):\n",
    "    df_returns = pd.DataFrame()\n",
    "    df_returns['date'] = df['date']\n",
    "    for col in df.columns[1:]:\n",
    "        df_returns[col] = df[col].pct_change()\n",
    "    return df_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct = create_daily_returns_matrix(df_svd).fillna(0)\n",
    "df_pct.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_pct['date'], df_pct['gold'], alpha=0.3, label='gold')\n",
    "plt.plot(df_pct['date'], df_pct['pal'], alpha=0.3, label='pal')\n",
    "plt.plot(df_pct['date'], df_pct['plat'], alpha=0.3, label='plat')\n",
    "plt.plot(df_pct['date'], df_pct['silver'], alpha=0.3, label='silver')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorized = df_pct.iloc[:, 1:]\n",
    "covariance = np.matmul(vectorized.T, vectorized)\n",
    "covariance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(covariance)\n",
    "eig_vecs = eig_vecs.T\n",
    "sort_indices = eig_vals.argsort()[::-1]\n",
    "eig_vals = eig_vals[sort_indices]\n",
    "eig_vecs = eig_vecs[sort_indices]\n",
    "\n",
    "eig_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "After decomposing our covariance matrix, we get the above eigenvalues and eigenvectors. \n",
    "\n",
    "Each eigenvalue corresponds to the how much of the total variance of the data the corresponding principal component captures. \n",
    "\n",
    "The corresponding principal component represents a portfolio allocation that captures that amount of variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(starting_alloc, starting_cash, df_pct, years=10, portfolio_name='10 Years Eigenportfolio', ax=None):\n",
    "    # Using a risk-free rate\n",
    "    ffr = 2\n",
    "    alloc = [starting_cash]\n",
    "    \n",
    "    rel_returns = (df_pct.iloc[:, 1:]).copy(deep=True)\n",
    "    for idx, item in enumerate(starting_alloc):\n",
    "        sign = 1 if item > 0 else -1\n",
    "        rel_returns.iloc[:, idx] = 1 + sign * rel_returns.iloc[:, idx]\n",
    "        \n",
    "    alloc = rel_returns.cumprod().multiply(np.absolute(starting_alloc), axis=1).sum(axis=1)\n",
    "    # Plot returns\n",
    "    if not ax:\n",
    "        ax = plt.figure(figsize=(12, 8)).gca()\n",
    "    ax.plot(df_pct['date'], alloc, label=portfolio_name, alpha=0.5)\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Portfolio Value')\n",
    "    ax.set_title(portfolio_name)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Sharpe and Sortino Ratio\n",
    "    port_returns= alloc.pct_change() * 100\n",
    "    std_returns = np.std(port_returns) * math.sqrt(252)\n",
    "    sortino_returns = np.std(port_returns.where(port_returns < 0)) * math.sqrt(252)\n",
    "    tot_returns = (alloc.iloc[-1] - starting_cash)/alloc[0]\n",
    "    sign_returns = 1 if tot_returns > 0 else -1\n",
    "    \n",
    "    annual_return = calc_annualized_return(np.absolute(tot_returns), years)* 100\n",
    "    sharpe = sign * (annual_return - ffr)/std_returns\n",
    "    sortino = sign * (annual_return - ffr) / sortino_returns\n",
    "    print('Portfolio: %s' % portfolio_name)\n",
    "    print('Sharpe Ratio: %0.3f' % sharpe)\n",
    "    print('Sortino Ratio: %0.3f' % sortino)\n",
    "    \n",
    "    return alloc, rel_returns, sharpe, sortino\n",
    "\n",
    "def calc_annualized_return(cum_return, years):\n",
    "    tot_return = 1+cum_return\n",
    "    return math.e ** (math.log(tot_return)/years)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Investing in the first principal eigenportfolio approximates the market as a whole. \n",
    "first_pc = eig_vecs[0]\n",
    "first_pc = first_pc / np.sum(np.absolute(first_pc))\n",
    "\n",
    "# Backtesting and comparing the weighted Eigenportfolio and an equal weight portfolio. \n",
    "\n",
    "starting_cash = 1000\n",
    "principal_alloc = starting_cash * first_pc\n",
    "equal_alloc = starting_cash * np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "ax = plt.figure(figsize=(12, 8)).gca()\n",
    "\n",
    "_, _, _, _ = backtest(principal_alloc, starting_cash, df_pct, years=10, portfolio_name='10 Years Principal Eigenportfolio', ax=ax)\n",
    "_, _, _, _ = backtest(equal_alloc, starting_cash, df_pct, years=10, portfolio_name='10 Years Equal Weight Portfolio', ax=ax)\n",
    "\n",
    "ax.grid()\n",
    "ax.set_title('Principal Eigenportfolio vs. Market', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krein's Theorem\n",
    "When all assets have non-negative correlation, the coefficients of the first principal component are all non-negative. \n",
    "\n",
    "Therefore, the first principal component is the portfolio allocation that corresponds to market forces. The corresponding eigenvalue shows how much of the total variance of the data is explained by market forces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(np.arange(0, 4), eig_vals/sum(eig_vals), color='red')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Eigenvector', fontsize=14)\n",
    "plt.ylabel('Percent of Variance Explained', fontsize=14)\n",
    "plt.title('Spectral Analysis', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal Eigenvectors\n",
    "\n",
    "It follows that all other eigenportfolios of the eigenbasis is orthogonal to the first principal component. This means that it's \"uncorrelated\" with the market, and hence, market neutral. \n",
    "\n",
    "Furthermore, since the principal component has all positive coefficients, the orthogonal eigenvectors must have at least 1 negative coefficient. \n",
    "\n",
    "Therefore, we claim that the orthogonal eigenvectors represent a market neutral pairs trading strategy that aims to capture the variance not explained by the market. \n",
    "\n",
    "This variance is comprised of intercompany reasons and noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "starting_cash = 1000\n",
    "second_eig_alloc = eig_vecs[1]\n",
    "second_eig_alloc = second_eig_alloc/sum(np.absolute(second_eig_alloc)) * 1000\n",
    "\n",
    "_, _, _, _ = backtest(second_eig_alloc, starting_cash, df_pct, years=10, portfolio_name='10 Years 2nd Eigenportfolio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting\n",
    "\n",
    "We will now try to backtest this algorithm by training on the first 8 years of historical data and then testing it on the final 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pct.iloc[4500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorized_bt = df_pct.iloc[:4500, 1:]\n",
    "covariance_bt = np.matmul(vectorized_bt.T, vectorized_bt)\n",
    "covariance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(covariance_bt)\n",
    "eig_vecs = eig_vecs.T\n",
    "sort_indices = eig_vals.argsort()[::-1]\n",
    "eig_vals = eig_vals[sort_indices]\n",
    "eig_vecs = eig_vecs[sort_indices]\n",
    "\n",
    "eig_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the Eigenvectors corresponding to the daily returns of this basket of commodities, we will try to interpret them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Investing in the first principal eigenportfolio approximates the market as a whole. \n",
    "first_pc = eig_vecs[0]\n",
    "first_pc = first_pc / np.sum(np.absolute(first_pc))\n",
    "\n",
    "# Backtesting and comparing the weighted Eigenportfolio and an equal weight portfolio. \n",
    "\n",
    "starting_cash = 1000\n",
    "principal_alloc = starting_cash * first_pc\n",
    "equal_alloc = starting_cash * np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "ax = plt.figure(figsize=(12, 8)).gca()\n",
    "\n",
    "_, _, _, _ = backtest(principal_alloc, starting_cash, df_pct.iloc[:4500], years=10, portfolio_name='8 Years Principal Eigenportfolio', ax=ax)\n",
    "_, _, _, _ = backtest(equal_alloc, starting_cash, df_pct[:4500], years=10, portfolio_name='8 Years Equal Weight Portfolio', ax=ax)\n",
    "\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(np.arange(0, 4), eig_vals/sum(eig_vals), color='red')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel('Eigenvector', fontsize=14)\n",
    "plt.ylabel('Percent of Variance Explained', fontsize=14)\n",
    "plt.title('Spectral Analysis', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct.iloc[4500:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "starting_cash = 1000\n",
    "second_eig_alloc = eig_vecs[1]\n",
    "second_eig_alloc = second_eig_alloc/sum(np.absolute(second_eig_alloc)) * 1000\n",
    "\n",
    "ax = plt.figure(figsize=(12, 8)).gca()\n",
    "\n",
    "_, _, _, _ = backtest(second_eig_alloc, starting_cash, df_pct.iloc[4500:].reset_index(drop=True), years=2, portfolio_name='Eigenportfolio', ax=ax)\n",
    "_, _, _, _ = backtest(equal_alloc, starting_cash, df_pct.iloc[4500:].reset_index(drop=True), years=2, portfolio_name='Equal Allocation', ax=ax)\n",
    "\n",
    "ax.grid()\n",
    "ax.set_title('Eigenportfolio vs. Equal Allocation Portfolio', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenportfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eig_vecs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Picture\n",
    "\n",
    "* This technique is an interesting way to decompose a sector and potentially find market neutral pairs trading opportunities between different assets of a sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveats and Next Steps\n",
    "\n",
    "* Volatility effects the eigenportfolios by a lot. We need to explore the effects of normalizing by volatility. \n",
    "\n",
    "* This assumes the market forces are linear. Will need to investigate results by using non-linear PCA\n",
    "\n",
    "* This is constrained to groups of assets where all assets are correlated with each other to some degree. \n",
    "\n",
    "* Will need to look at backtesting results when eigenportfolios are dynamically updated.\n",
    "\n",
    "* Assumes past trends will predict future trends\n",
    "\n",
    "\n",
    "* look at very stable markets, medical equipment, construction, retail, semiconductor, things that are robust to policy related volatility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
